# Fraud-Detector-App
üîç Job Fraud Detector | Anveshan Hackathon Project An ML-powered Streamlit web app that detects fake job listings using natural language processing, feature engineering, and XGBoost. Includes real-time predictions, SHAP explanations, search, filtering, and CSV export ‚Äî built to protect job seekers from online scams

training data csv:- https://drive.google.com/file/d/1oxygynnHOAU3NU3S8xHRdBS9WA_K1E7y/view?usp=sharing

Fraud Detector: Smart Job Scam Detection System

üß† Project Overview Online job platforms are frequently targeted by scammers. These fake job listings waste applicants‚Äô time and put their personal information at risk.

This project builds a machine learning pipeline that automatically detects fraudulent job listings and presents insights in a Streamlit-powered interactive dashboard.

üéØ Problem Statement Develop a system to classify job listings as fraudulent or genuine using structured and unstructured data. Display predictions and insights in an intuitive dashboard.

üöÄ Automatic Job Fraud Detection Classifies job listings as real or fake using advanced machine learning techniques.

üß† Hybrid Feature Engineering Combines TF-IDF text vectors with structured numeric features:

‚Ä¢ Description length & word count

‚Ä¢ Suspicious keywords (e.g., money, bitcoin)

‚Ä¢ Free email domain indicators (e.g., gmail.com, yahoo.com)

‚Ä¢ Digit count in job titles

üå≤ XGBoost Classifier for Accuracy & Speed

‚Ä¢ Leverages the power of XGBoost ‚Äî a high-performance gradient boosting algorithm ‚Äî for fast training and accurate predictions.

‚öñÔ∏è Class Imbalance Handling with SMOTE

‚Ä¢ Uses SMOTE oversampling to improve detection of rare fraudulent cases without overfitting.

üß™ F1-Optimized Threshold Tuning

Dynamically selects the best threshold using precision-recall curve to maximize the F1-score ‚Äî ideal for imbalanced data.

üìä Visual Analytics Dashboard

‚Ä¢ Histogram of fraud probabilities

‚Ä¢ Pie chart of predicted real vs. fake listings

‚Ä¢ Threshold vs. F1-score visualization

‚Ä¢ SHAP summary plots for feature importance

‚Ä¢ Explainable AI with SHAP

-Visualize top contributing features

-Interpret individual predictions

-Enhance transparency and trust in model decisions

‚úâÔ∏è High-Risk Job Alert System

Automatically flags listings with fraud probability ‚â• 0.8 and supports email alerts via SMTP integration.

üîó Unified Text + Numeric Feature Pipeline

Combines TF-IDF matrix with custom features using scipy.sparse.hstack for seamless model input.

‚ö° Deployment-Ready with Streamlit

Integrated with a user-friendly Streamlit interface for real-time fraud prediction and insights.

üõ†Ô∏è Technologies Used

Area	Tools/Libraries
Data Handling	pandas, numpy
Modeling	XGBoost, SMOTE
NLP	TfidfVectorizer
Evaluation	F1-score, precision-recall curve
Visualization	matplotlib, seaborn, SHAP
Web Dashboard	Streamlit
Deployment	Streamlit Cloud
üßæ Setup Instructions

image

‚Ä¢ pandas and numpy: Used for handling structured data and numerical operations.

‚Ä¢ matplotlib.pyplot and seaborn: Used for plotting graphs and visualizing distributions and model results.

‚Ä¢ TfidfVectorizer: Converts text into numerical features based on frequency and importance.

‚Ä¢ RandomForestClassifier: (Though unused here) A machine learning model. You used XGBClassifier instead.

‚Ä¢ XGBClassifier: An optimized version of gradient-boosted trees, very powerful for tabular data.

‚Ä¢ train_test_split: Splits data into training and validation sets.

‚Ä¢ classification_report, confusion_matrix, f1_score, etc.: Used to evaluate model performance.

‚Ä¢ SMOTE: Balances the dataset by generating synthetic examples for the minority class (fraudulent jobs).

‚Ä¢ shap: Helps explain the model‚Äôs predictions and shows which features are most important.

‚Ä¢ warnings.filterwarnings("ignore"): Suppresses warnings for clean output.

image

‚Ä¢ Reading CSV files into pandas DataFrames.

‚Ä¢ on_bad_lines='skip' ensures any corrupted lines or format errors are skipped rather than crashing the program.

image

Dataset Preparation & Feature Engineering To ensure robust and consistent model training, we preprocess and enrich the dataset through a series of thoughtful transformations:

Safe Copy of the Data ‚Ä¢ We begin by copying the input DataFrame to avoid modifying the original data. This is a best practice for all preprocessing pipelines.

Validate and Clean Text Columns ‚Ä¢ We ensure key textual columns (title, description, requirements, company_profile) exist. If missing, they're created and filled with empty strings to avoid errors during vectorization.

Clean the Email Column ‚Ä¢ If the email field exists, we replace all missing values with blank strings to maintain formatting and prevent errors during feature extraction.

Merge Text for TF-IDF

We combine all main text fields into a unified full_text column, which is used to generate TF-IDF features. This leverages the entire job listing content for better text-based learning.

Generate Custom Features

‚Ä¢ We engineer several insightful features to detect fraud patterns:

‚Ä¢ desc_len: Length of the job description (in characters).

 ‚Ä¢ word_count: Number of words in the description.

‚Ä¢ num_digits_in_title: Count of numerical digits in the job title (e.g., ‚ÄúEarn $5000‚Äù).

‚Ä¢ has_profile: Boolean flag ‚Äî does the company provide a profile?

‚Ä¢ suspicious_terms: Checks for keywords like "bitcoin", "transfer", "click", "money", etc., common in scams.
Email-Based Features

email_domain: Extracted from the email (e.g., gmail.com, company.com).

free_email: Flags whether the domain belongs to a free provider (like gmail, yahoo, etc.), often used in fake listings.

Consistency Across Datasets We apply the same transformation function to both training and test datasets. This guarantees consistent features during model training and inference.
image

Split the dataset:

‚Ä¢ X: Input features from the training data.

‚Ä¢ y: Output labels (0 = Real, 1 = Fraudulent).

‚Ä¢ X_test: Input features from the test data (labels are not available).
Features in X:

‚Ä¢ Text feature: Combined text column (formed by concatenating title, description, requirements, company_profile).

‚Ä¢ Handcrafted numeric features:

‚Ä¢ desc_len: Number of characters in the job description.

‚Ä¢ word_count: Number of words in the description.

‚Ä¢ num_digits_in_title: Number of digits in the job title.

‚Ä¢ has_profile: Binary flag indicating presence of a company profile.

‚Ä¢ suspicious_terms: Binary flag if scammy terms like money, bitcoin exist.

‚Ä¢ free_email: Binary flag for free email domains (e.g., gmail.com).
TF-IDF Vectorization

‚Ä¢ To handle text data, we use TF-IDF (Term Frequency-Inverse Document Frequency) ‚Äî a statistical method to evaluate how important a word is in a document relative to the entire corpus

Configuration:

‚Ä¢ max_features = 5000: Use the 5000 most informative words.

‚Ä¢ stop_words = 'english': Removes common unimportant words (e.g., the, and, is).
Apply TF-IDF:

‚Ä¢ On X['text'] ‚Üí creates X_tfidf (sparse matrix).

‚Ä¢ On X_test['text'] ‚Üí creates X_test_tfidf (using the same vocabulary as training).
Combining All Features

The TF-IDF matrix (text features),

With 6 handcrafted numerical features (from feature engineering).

We use scipy.sparse.hstack() to horizontally stack these two sets of features into one big feature matri.

Final matrices:

‚Ä¢ X_combined: Final feature set for training.

‚Ä¢ X_test_combined: Final feature set for test predictions.
‚Ä¢ This combined matrix is now ready to feed into machine learning models like XGBoost giving the model both:

These combined matrices are then used to train ML models like XGBoost or Random Forest, benefiting from:

‚Ä¢ Rich semantic content (from text)

‚Ä¢ Strong rule-based signals (from numeric features
image

‚Ä¢ Handling Class Imbalance and Model Training

Problem: Class Imbalance

‚Ä¢ Fraudulent job postings are rare.

‚Ä¢ This causes class imbalance ‚Äî a major challenge in machine learning.

‚Ä¢ A model that always predicts ‚Äúnot fraud‚Äù may achieve high accuracy but is useless.
Solution: SMOTE (Synthetic Minority Oversampling Technique)

‚Ä¢ SMOTE creates synthetic fraud examples by interpolating between real ones.

‚Ä¢ This balances the dataset and helps the model learn fraud patterns more effectively.
Train-Validation Split

‚Ä¢ Dataset is split using train_test_split() with stratify=y to maintain class distribution.
Model: XGBoost Classifier

‚Ä¢ Chosen for its speed and accuracy in classification tasks.
‚Ä¢ Key hyperparameters:

 ‚Ä¢ n_estimators=200: Number of trees.

 ‚Ä¢ max_depth=6: Controls model complexity to avoid overfitting.

 ‚Ä¢ subsample & colsample_bytree: Randomly sample rows and features to enhance generalization.

 ‚Ä¢ eval_metric='logloss': Metric to evaluate binary classification error.
Training

‚Ä¢ The model is trained on SMOTE-balanced data: X_res, y_res.

image

‚Ä¢ Threshold Optimization for Better Performance

Default Threshold Issue

‚Ä¢ Most models use a default threshold of 0.5 for binary classification.

 ‚Ä¢ On imbalanced datasets, this may not yield optimal results.
Threshold Tuning

‚Ä¢ Use precision_recall_curve to compute scores across all possible thresholds.

‚Ä¢ Calculate F1 Score (harmonic mean of precision and recall) for each threshold.

‚Ä¢ Select the best_threshold that maximizes F1 Score.
Plot F1 vs Threshold

‚Ä¢ Helps visualize how F1 score changes with different classification thresholds.

‚Ä¢ Aids in selecting the optimal best_threshold.
Final Predictions with Best Threshold

‚Ä¢ Apply best_threshold to the model's output probabilities.

‚Ä¢ Generate predictions for evaluation.
Evaluate with classification_report()

‚Ä¢ Provides detailed performance metrics:

‚Ä¢ Precision: Of all predicted frauds, how many were actually fraud?

‚Ä¢ Recall: Of all actual frauds, how many did the model detect?

‚Ä¢ F1 Score: Harmonic mean of precision and recall ‚Äî balances false positives and false negatives.

‚Ä¢ Support: Number of true instances in each class.
Evaluate with confusion_matrix()
‚Ä¢ Breaks down predictions into four categories:

    ‚Ä¢ True Positives (TP) ‚Äì Correctly predicted fraud cases.

    ‚Ä¢ True Negatives (TN) ‚Äì Correctly predicted genuine jobs.

    ‚Ä¢ False Positives (FP) ‚Äì Genuine jobs incorrectly labeled as fraud.

    ‚Ä¢ False Negatives (FN) ‚Äì Fraud cases the model failed to detect
‚Ä¢ This helps us understand model performance beyond accuracy and ensures it catches fraud with minimal false alarms.

image

Now that our model is trained and fine-tuned, we use it to predict the fraud probability of each job listing in the test dataset.

Use the Trained Model to Predict Fraud Probabilities:

 ‚Ä¢ model.predict_proba(X_test_combined)[:, 1]
 ‚Üí Returns the probability of class 1 (fraud) for each job listing in the test set.
Store Probabilities:

‚Ä¢ Save the output as a new column:
   test_df['fraud_probability']
Convert Probabilities to Binary Predictions:

‚Ä¢ Apply the previously selected best_threshold:

 ‚Ä¢ If probability ‚â• threshold ‚Üí fraud_predicted = 1 (high likelihood of fraud)

 ‚Ä¢ If probability < threshold ‚Üí fraud_predicted = 0 (likely genuine)
‚Ä¢ Save this as: test_df['fraud_predicted']

Result:
‚Ä¢ A clean column with 0s and 1s representing model predictions for fraud.

üìä Visualizing Model Confidence 5. Histogram of Fraud Probabilities:

  ‚Ä¢ Plot all values in test_df['fraud_probability']

  ‚Ä¢ Peaks near 0 ‚Üí Model is confident many jobs are real.

  ‚Ä¢ Peaks near 1 ‚Üí Model has high confidence in fraud predictions.
KDE Line (Smooth Curve):

‚Ä¢ Added to the histogram to better visualize the shape of the probability distribution.
‚Ä¢ Visualizing Fraud vs Real Predictions

Pie Chart of Predicted Labels:

‚Ä¢ Use values from test_df['fraud_predicted'] to count real vs fake predictions.

‚Ä¢ Provides a clear view of the percentage of job listings flagged as fake.
‚Ä¢ Helps decision-makers or investigators understand how widespread fraud is in the test dataset according to the model.

image

‚Ä¢ Why SHAP?

‚Ä¢ SHAP (SHapley Additive exPlanations) is a method from game theory used to explain individual predictions made by machine learning models. It assigns each feature a contribution score (positive or negative) toward the final prediction.

Dense Conversion for SHAP Compatibility

‚Ä¢ SHAP requires input data in dense format.
‚Üí We convert a subset of the training data (e.g., X_res[:100]) using .toarray().
SHAP Explainer Setup

‚Ä¢ We create a SHAP explainer using the trained XGBoost model and the dense data.
‚Üí This explainer understands the model logic and computes contribution scores.
SHAP Values Computation

‚Ä¢ SHAP values tell how much each feature pushes a prediction toward class 1 (fraud) or class 0 (non-fraud).

Feature Names
‚Ä¢ We combine:

     ‚Ä¢ Top 5000 TF-IDF features from job descriptions and titles

     ‚Ä¢ Custom-engineered features like:

     ‚Ä¢ desc_len

     ‚Ä¢ word_count

     ‚Ä¢ num_digits_in_title

     ‚Ä¢ has_profile

     ‚Ä¢ suspicious_terms

     ‚Ä¢ free_email
Summary Plot Visualization
‚Ä¢ We generate a SHAP summary plot which shows:

‚Ä¢ Most influential features

‚Ä¢ Direction (does a feature increase or decrease fraud risk?)

‚Ä¢ Distribution and strength of these features across samp
image

High-Risk Job Listings Alert

‚Ä¢ While your model predicts all fraud probabilities, not every ‚Äúfraudulent‚Äù prediction has the same confidence. A job predicted at 0.51 might just barely cross the threshold ‚Äî but one predicted at 0.95 is likely very suspiciou

Why This Matters
‚Ä¢ Jobs with very high fraud probability (e.g., 0.95) are more likely to be genuinely fraudulent and may require:

 ‚Ä¢ Human verification or moderation

 ‚Ä¢ Platform filtering/blocking

 ‚Ä¢ Alerts to internal teams
Implementation Overview
‚Ä¢ Thresholding A cut-off of 0.80 is used to flag high-confidence frauds. (This value can be adjusted based on acceptable risk tolerance.)

‚Ä¢ Filtering From the test dataset, all listings with fraud_probability ‚â• 0.8 are filtered into a separate DataFrame.

‚Ä¢ Formatted Alert For each high-risk listing, an alert message is generated with:

‚Ä¢ Title

‚Ä¢ Location

‚Ä¢ Fraud Probability (rounded to 2 decimal places)
üì® OUTPUT BASED ON SAMPLE INPUT

image

F1 Score Optimization

The F1 Score combines Precision and Recall into a single metric, making it ideal for imbalanced datasets like fraud detection where accuracy alone can be misleading. image

Threshold Analysis

‚Ä¢ X-axis: Probability thresholds for classification (from 0 to 1)

‚Ä¢ Y-axis: F1 Score achieved at each threshold

‚Ä¢ A blue curve shows how the F1 Score changes across different thresholds. ‚Ä¢ A red dashed vertical line marks the best-performing threshold.

Key Results ‚Ä¢ Best Threshold: 0.21

    ‚Ä¢ Instead of the standard 0.5 threshold, our model performs best when classifying any instance with a fraud probability above 21% as fraudulent.
‚Ä¢ Peak F1 Score: ~0.81

    ‚Ä¢ This is the highest achievable balance between precision and recall with our current model.l
Why such a low threshold?

‚Ä¢ our model tends to predict conservative (low) probabilities even for actual fraud cases, so using a lower cutoff helps catch more fraudulent transactions while maintaining reasonable precision
Implementation Impact

‚Ä¢ This threshold optimization significantly improves model performance by:

  ‚Ä¢ Increasing Recall - Catching more actual fraud cases

  ‚Ä¢ Maintaining Precision - Avoiding excessive false positives

  ‚Ä¢ Maximizing F1 Score - Achieving optimal balance for our imbalanced dataset
image

‚Ä¢ Threshold Tuning

‚Ä¢ To optimize fraud detection, we tuned the classification threshold instead of using the default 0.5. The best performance was achieved at threshold = 0.21, giving the highest F1 Score of 0.8109 ‚Äî effectively balancing precision and recall.

 Threshold	         F1 Score
  0.10	           0.7230
  0.21	           0.8109 
0.25‚Äì0.50	         ~0.78‚Äì0.79
‚Ä¢ Final Performance (Validation Set)

 Metric                   Value                     Interpretation                                    

‚Ä¢ Accuracy                 0.98                        Overall correct predictions.                      
‚Ä¢ Precision (Fraud)        0.78                        78% of fraud predictions were correct.            
‚Ä¢ Recall (Fraud)           0.85                        85% of actual frauds were detected.               
‚Ä¢ F1 Score (Fraud)         0.81                        Balanced metric of fraud detection.               
‚Ä¢ Macro Avg (F1)           0.90                        Treats both classes equally.                      
‚Ä¢ Weighted Avg (F1         0.98                        Reflects performance considering class imbalance. 
‚Ä¢ Confusion Matrix

                          Predicted Real	                 Predicted Fraud
‚Ä¢ Actually Real	         2688 	                          34 (False Positives)
‚Ä¢ Actually Fraud	        21 (False Negatives)	           118 
‚Ä¢ Key Insights

‚Ä¢ Only 21 fraud cases were missed (false negatives).

‚Ä¢ Only 34 real jobs were wrongly flagged as fraud (false positives).
‚Ä¢ Most classifications are correct ‚Äî this is a well-performing, balanced model.

image

Key Insights from the Fraud Probability Distribution

Distribution Pattern:
What This Graph Tells Us

‚Ä¢ X-axis: Fraud Probability The probability output by your model that a job posting is fraudulent (ranges from 0 to 1).

Closer to 0 = more likely real Closer to 1 = more likely fraud

‚Ä¢ Y-axis: Frequency How many samples fall into each probability range/bin

‚Ä¢ Most predictions clustered near 0 (legitimate jobs)

‚Ä¢ Small cluster near 1.0 (fraudulent jobs)

‚Ä¢ Sparse middle values indicate decisive model behavior
Model Behavior:

‚Ä¢ High confidence predictions - model rarely assigns intermediate probabilities (0.4-0.6)

‚Ä¢ Strong feature separability - clear distinction between fraud and legitimate postings

‚Ä¢  The pattern reflects a class imbalance‚Äîfraudulent jobs are rare compared to legitimate ones.

‚Ä¢ A smoothed KDE curve (red line) highlights the underlying distribution, confirming the two peaks (at 0 and 1).
Practical Implications:

‚Ä¢ Low threshold justified - setting threshold around 0.21 captures frauds in the right tail

‚Ä¢ High threshold problematic - using 0.5+ would miss many actual frauds

‚Ä¢ The model is reliable for the majority class (legitimate jobs), and also identifies true frauds at the far right.
image

What This Pie Chart Shows

Title: ‚ÄúPredicted Fake vs Real Job Listings‚Äù

Prediction Breakdown:

‚Ä¢ 94.8% of job listings are predicted as real (light blue).

‚Ä¢ 5.2% are predicted as fraudulent (salmon red).
Class Imbalance:

‚Ä¢ The chart reflects real-world data where most jobs are genuine.
Fraud Detection Strategy:

‚Ä¢ 5.2% is significantly higher than the true base rate of fraud in most datasets (usually <2%), meaning your model is trying to err on the side of caution, which is a good strategy in fraud detection.
Practical Implication:

‚Ä¢ It‚Äôs likely catching more fraud at the cost of a few false positives, which is usually acceptable in fraud screening systems.
‚Ä¢ This cautious approach is typical and effective in fraud detection systems
image

Feature Importance Order:

‚Ä¢ Features are listed top-to-bottom by their average impact on predictions (most important at the top).
SHAP Value (X-axis):

‚Ä¢ Positive SHAP values: Push prediction toward fraud (class = 1).

‚Ä¢ Negative SHAP values: Push prediction toward real (class = 0).
Dots (Individual Predictions):

‚Ä¢ Each dot represents one job listing.

‚Ä¢ Dot position shows how much that feature influenced the fraud prediction for that listing.
Color (Feature Value):

‚Ä¢ Red/pink = High feature value.

‚Ä¢ Blue = Low feature value.
Key Feature Insights:

‚Ä¢ has_profile:

       ‚Ä¢ Most influential feature.

       ‚Ä¢ High value (red, likely ‚Äúno profile‚Äù) strongly pushes toward fraud.

       ‚Ä¢ Low value (blue, ‚Äúhas profile‚Äù) pushes toward real.

       ‚Ä¢ word_count & desc_len:

       ‚Ä¢ High values (red, longer descriptions) push toward real jobs.

       ‚Ä¢ Low values (blue, short descriptions) push toward fraud.
‚Ä¢ Keyword Features (e.g., fun, goal, team, years):

  ‚Ä¢ Words like ‚Äúfun‚Äù and ‚Äúgoal‚Äù (red) tend to push toward fraud‚Äîpossibly because fake jobs use vague, motivational language.

  ‚Ä¢  Words like ‚Äúteam,‚Äù ‚Äúyears,‚Äù and ‚Äúsolutions‚Äù push toward real‚Äîsuggesting structured, corporate language signals legitimacy.
üöÄ Live Demo

‚Ä¢ Click here to open the Streamlit App

‚Ä¢ Features

  ‚Ä¢ Fraud Detection  using XGBoost, TF-IDF, and SMOTE
  ‚Ä¢ Interactive UI Design built with Streamlit
  ‚Ä¢ Custom Threshold Slider to tune sensitivity
  ‚Ä¢ Live Metrics Dashboard with precision, recall, F1-score, and confusion matrix
  ‚Ä¢ Search & Filter functionality for job posts
  ‚Ä¢ Visualizations Pie charts and histograms for data insights
  ‚Ä¢ Downloadable Results in CSV format
THIS IS THE Also the link:-

https://fraud-detector-app-cgr9btenw5tzsp72fpv2gu.streamlit.app/

photos of app we have designed

UI DESIGN OF THE APP

image

output after browsing the file

image

image

üìÑ fraud_app (12).py
This script contains the complete Streamlit code for deploying the fraud detection app, including model loading, threshold tuning, performance metrics, and interactive UI components.

üìî Colab Notebook
fraud_app.ipynb you can excess the code from here

This notebook includes model training with TF-IDF, SMOTE, XGBoost, and evaluation before deployment.

since we have to speed up the video in order to compressed to 8mins.so we kindly request to pls watch this video at speed 0.5x for better viewing experience.pls consider our request as we have put enormous amount of work into completion of this project

üé• [Watch the presentation video]

‚Äî Recommended speed: 0.5x for the best viewing experience.

About
üîç Job Fraud Detector | Anveshan Hackathon Project An ML-powered Streamlit web app that detects fake job listings using natural language processing, feature engineering, and XGBoost. Includes real-time predictions, SHAP explanations, search, filtering, and CSV export ‚Äî built to protect job seekers from online scams

Resources
 Readme
 Activity
Stars
 0 stars
Watchers
 0 watching
Forks
 0 forks
Report repository
Releases
No releases published
Packages
No packages published
Languages
Jupyter Notebook
93.7%
 
Python
6.3%
Footer
